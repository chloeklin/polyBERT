{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import random\n",
    "from transformers import AutoTokenizer, AutoModel, AutoModelForMaskedLM, DebertaV2ForMaskedLM, DebertaV2Tokenizer\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-1.5.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\n",
      "Requirement already satisfied: numpy>=1.19.5 in /scratch/hm62/hl4138/polybert-venv/lib/python3.9/site-packages (from scikit-learn) (2.0.2)\n",
      "Collecting scipy>=1.6.0 (from scikit-learn)\n",
      "  Downloading scipy-1.13.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
      "Collecting joblib>=1.2.0 (from scikit-learn)\n",
      "  Downloading joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn)\n",
      "  Downloading threadpoolctl-3.5.0-py3-none-any.whl.metadata (13 kB)\n",
      "Downloading scikit_learn-1.5.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.4/13.4 MB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "Downloading scipy-1.13.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (38.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.6/38.6 MB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading threadpoolctl-3.5.0-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: threadpoolctl, scipy, joblib, scikit-learn\n",
      "Successfully installed joblib-1.4.2 scikit-learn-1.5.2 scipy-1.13.1 threadpoolctl-3.5.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"/scratch/hm62/hl4138/polyBERT/polyBERT/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrain_sizes = ['1M','5M','10M','20M','50M','90M']\n",
    "size = '1M'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_masked_test_set(tokenizer, sentences, mask_prob=0.15):\n",
    "    masked_sentences = []\n",
    "    ground_truth = []\n",
    "\n",
    "    for sentence in sentences:\n",
    "        tokenized_input = tokenizer.tokenize(sentence)\n",
    "        masked_sentence = tokenized_input.copy()  # Copy of tokenized sentence\n",
    "        ground_truth_sentence = []\n",
    "\n",
    "        for i, token in enumerate(tokenized_input):\n",
    "            if random.random() < mask_prob:  # Mask with a certain probability (e.g., 15%)\n",
    "                ground_truth.append(token)  # Store original token\n",
    "                masked_sentence[i] = tokenizer.mask_token  # Replace with [MASK]\n",
    "\n",
    "        masked_sentences.append(tokenizer.convert_tokens_to_string(masked_sentence))\n",
    "        # ground_truth.append(ground_truth_sentence)\n",
    "\n",
    "    return masked_sentences, ground_truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU availability\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "torch.cuda.is_available() #checking if CUDA + Colab GPU works\n",
    "\n",
    "# Load test dataset\n",
    "file_path = 'data/generated_polymer_smiles_dev.txt'\n",
    "\n",
    "with open(file_path, 'r') as file:\n",
    "    psmiles_strings = [line.strip() for line in file]\n",
    "\n",
    "psmiles_strings = psmiles_strings[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[*]c1cccc(P(=O)(c2ccccc2)c2cccc(-c3nc4ccc(-c5ccccc5)cc4nc3N3C(=O)c4ccc(-c5ccc6c(c5)C(=O)N([*])C6=O)cc4C3=O)c2)c1'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "psmiles_strings[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[MASK][MASK] 1cccc[MASK] p(=o)([MASK] 2[MASK] cccc[MASK] )c2ccc[MASK] (-c3nc4[MASK] cc(-c5ccccc5)cc4nc3n[MASK] c([MASK] o[MASK] c4ccc(-c5c[MASK] c6[MASK] (c5)c(=[MASK] )[MASK][MASK][*])c6=o)cc4c3=o)c2)c1',\n",
       " '[*]oc1ccc[MASK] -c2ccc(c)c(-c3[MASK] c(-c4cc(n5c([MASK] o)c[MASK] (=[MASK] )n([*])[MASK] 5=s)ccc4c)[MASK] cc[MASK] c)c2[MASK] cc[MASK]',\n",
       " '[*]oc1ccc(-c2ccc3c(c2)c[MASK][MASK] cc(oc([*])=o)cc[MASK][MASK] -3)[MASK]1',\n",
       " '[*][MASK] (=o)c1(cc)[MASK] cc(oc[MASK] cc[MASK] c(n3c(=o)[MASK] 4ccc([*])c[MASK] 4c3=o)[MASK]2)cc1',\n",
       " '[MASK] os[MASK][MASK] o)(=o[MASK] c1[MASK] c[MASK] (-c[MASK] cc[MASK] (-c3ccc[MASK] os([MASK] o)[MASK] =o)c4[MASK] cc(-c5ccc(-c6cc(br)c([*])c(br)c6)cc5)cc4)cc[MASK] )c([MASK])c2)cc1',\n",
       " '[MASK] oc1c[MASK] c[MASK] nc(-c[MASK] ccc4nc(n5c(=[MASK] )[MASK] 6cc[MASK][MASK] sc7cccc([*])c7[MASK] cc6c[MASK] =o)o[MASK] 4c3[MASK]oc2c1',\n",
       " '[MASK] c1sc(-c2[MASK] ([MASK][MASK] 3c(c)[MASK] c[MASK] -c[MASK] sc([MASK] c5[MASK] cc[MASK] c[MASK] )c[MASK] c[MASK] o[MASK] co5)cc3c([*])[MASK] c([MASK] )(f)f[MASK] c(f)(f)f[MASK] c3c2[MASK] cco[MASK])c2c1occo2',\n",
       " '[*]c[MASK][MASK] c[MASK][MASK] -c2[MASK] cc([MASK] /c=c[MASK] c(=o)c3ccc4nc(-c5ccccc[MASK] )cc(-c5cccc[MASK] [*])[MASK]5)c4c3)cc2)cc1',\n",
       " '[*]c[MASK] ccc2[MASK] (c1[MASK] c[MASK] =o)n[MASK] c1cc[MASK] (-c3ccc[MASK] 4c3c([MASK] o)n([*])c4=o)cc1-c1ccc3c(c1)c(=o)[MASK] (c1ccc(-c4ccc([n+]([MASK][MASK])[o-])cc4)cc1)c3=o)c2=o',\n",
       " '[*]c(=o)c1c(br)cc(s(=o)(=o)c2cc(br)c(-c3c(br)cc[MASK] s(=[MASK] )(=o)c4cc([MASK] )c(-c5c[MASK] c6c(c5)c(=o)[MASK] ([MASK] 5ccc([MASK] 7c[MASK][MASK][MASK] )c8ccc([*])cc8c7=o)cc5)c6=[MASK] )c([MASK] )[MASK] 4)[MASK] c3br)c(br[MASK][MASK][MASK])cc1br',\n",
       " '[*]o[MASK] 1ccc2c([MASK] 1)c(=o)n(c1ccc(-c3ccc4[nh]c(nc([MASK] no)c(=no)[MASK][MASK] 5ccc6[MASK] ([MASK][MASK] )c(=o)n(c5[MASK] cc([*])cc5)c[MASK] =o)nc4c3)cc1[MASK]c2=o',\n",
       " '[MASK] [*]oc1c[MASK] c(-c2c[MASK][MASK] cc[MASK] c2ccc(c)c[MASK] 2c2ccc(n=[MASK] c3ccc(c#[MASK] )cc3)cc2)cc1-c1ccc[MASK] [*])c[MASK]1',\n",
       " '[*]n1c([MASK] o)c2ccc(n3c(=o)c[MASK] c[MASK] c(n[MASK] c(=o)n([*][MASK] c(c)([MASK] 6c(=o)c7ccc(oc8ccc[MASK] n[MASK][MASK][MASK][MASK] cc[MASK] (-c[MASK] 10c[MASK] c(c#n)cc[MASK] 10)c[MASK] 9)cc8)cc7c6=o)c5=o)c[MASK] 4c3=o)cc2[MASK]1=o',\n",
       " '[*]c(=o[MASK] c1ccc(s[MASK][MASK][MASK] )(=o)c2c[MASK][MASK] (nc3ccc[MASK] s(=[MASK] )(=o)c4ccc(c5c[MASK] (=[MASK] )n(c[MASK] ccc(s(=[MASK] )[MASK] =o[MASK] c7ccc(n[MASK] c(=[MASK] )c9ccc([*])cc[MASK] c8=o)cc7)cc6)[MASK][MASK]=o)cc4)cc3)cc2)cc1',\n",
       " '[*]c[MASK][MASK] c2c[MASK] 3nc([MASK] ccc[MASK][MASK] (-c4ccc(-c5ccc(-c6ccc([[MASK] ]([*])(c)c)s6)s5)[MASK] c4)n[MASK] 3cc2nc1[MASK] n1[MASK] (=o)c2cc3[MASK] (=[MASK] )[nh]c([MASK]o)c3cc2c1=o',\n",
       " '[MASK] oc[MASK] ccc([MASK] c[MASK] ccc3[MASK] (c2)c(=o)n(c2ccc(c(c)(c)c4ccc([MASK] )s[MASK] )[MASK] c2)c3)[MASK]c1',\n",
       " '[*]oc1cc(n[MASK] 2c[MASK] c[MASK] (oc3nnc([*][MASK] o3)[MASK] 2)cc(c)c[MASK]c',\n",
       " '[*]oc(=o)c1[MASK] c[MASK] (nc2[MASK]c(=o)n([*])c2=o)cc1c',\n",
       " '[*]c1[MASK] cc[MASK] nc(=o)c2c[MASK] c3c[MASK] c2)c(=o)n[MASK] c2ccc([MASK] c4c[MASK][MASK][MASK] [*])[MASK] c4)[MASK] c2)c3=o[MASK]cc1',\n",
       " '[MASK] [*]c1ccc[MASK][MASK] c(-c3ccc4c([MASK] 3)c(=o)[MASK][MASK] c3cccc(n[MASK] c(=o[MASK] c6cc[MASK] (-c7nc8cc[MASK][MASK] [*])cc8o7)cc6c5=o)c3)[MASK] 4=o[MASK]oc2c1']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masked_psmiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[*]',\n",
       " 'c',\n",
       " '(',\n",
       " 'c',\n",
       " 'c',\n",
       " '2',\n",
       " 'c',\n",
       " 'c',\n",
       " '3',\n",
       " '=',\n",
       " ')',\n",
       " 'c',\n",
       " 'c',\n",
       " 'o',\n",
       " 'n',\n",
       " '(',\n",
       " '(',\n",
       " 'c',\n",
       " '=',\n",
       " 'c',\n",
       " 'o',\n",
       " 'c',\n",
       " 'c',\n",
       " '3',\n",
       " ')',\n",
       " '1',\n",
       " 'c',\n",
       " '2',\n",
       " 'c',\n",
       " '2',\n",
       " 's',\n",
       " 'c',\n",
       " 'c',\n",
       " '2',\n",
       " 'c',\n",
       " 'c',\n",
       " 'c',\n",
       " 'c',\n",
       " '[*]',\n",
       " '(',\n",
       " '=',\n",
       " ')',\n",
       " 'c',\n",
       " 'c',\n",
       " '2',\n",
       " 'c',\n",
       " '(',\n",
       " '=',\n",
       " '(',\n",
       " 'c',\n",
       " '3',\n",
       " 'cl',\n",
       " '[*]',\n",
       " 'c',\n",
       " '2',\n",
       " '3',\n",
       " 'o',\n",
       " 'c',\n",
       " 'c',\n",
       " '(',\n",
       " ')',\n",
       " '5',\n",
       " 'c',\n",
       " ')',\n",
       " '[*]',\n",
       " 'sc',\n",
       " '-',\n",
       " 'c',\n",
       " 'c',\n",
       " '(',\n",
       " '4',\n",
       " '-',\n",
       " 'c',\n",
       " 'c',\n",
       " '5',\n",
       " '5',\n",
       " '4',\n",
       " 'c',\n",
       " '(',\n",
       " 'f',\n",
       " ')',\n",
       " ')',\n",
       " 'o',\n",
       " '3',\n",
       " '1',\n",
       " 'c',\n",
       " 'c',\n",
       " '(',\n",
       " 'c',\n",
       " 'n',\n",
       " '\\\\',\n",
       " '5',\n",
       " '(',\n",
       " 'c',\n",
       " '1',\n",
       " 'c',\n",
       " ')',\n",
       " '(',\n",
       " '(',\n",
       " 'c',\n",
       " 'c',\n",
       " '=',\n",
       " 'n',\n",
       " '=',\n",
       " 'o',\n",
       " '(',\n",
       " 'o',\n",
       " 'br',\n",
       " 'c',\n",
       " 'n',\n",
       " 'c',\n",
       " 'n',\n",
       " '(',\n",
       " '=',\n",
       " 'o',\n",
       " 'o',\n",
       " 'br',\n",
       " 'c',\n",
       " 'c',\n",
       " ')',\n",
       " 'c',\n",
       " '2',\n",
       " 'c',\n",
       " 'c',\n",
       " '=',\n",
       " 'n',\n",
       " 'c',\n",
       " 'c',\n",
       " 'c',\n",
       " '5',\n",
       " 'c',\n",
       " '6',\n",
       " ')',\n",
       " '▁',\n",
       " 'c',\n",
       " 'c',\n",
       " 'c',\n",
       " '2',\n",
       " 'c',\n",
       " 'n',\n",
       " 'n',\n",
       " '(',\n",
       " 'c',\n",
       " '=',\n",
       " '4',\n",
       " 'c',\n",
       " '5',\n",
       " ')',\n",
       " 'n',\n",
       " '(',\n",
       " '=',\n",
       " 'n',\n",
       " 'c',\n",
       " '9',\n",
       " 'c',\n",
       " '%',\n",
       " 'c',\n",
       " '%',\n",
       " 'c',\n",
       " 'c',\n",
       " 'c',\n",
       " ')',\n",
       " '(',\n",
       " '=',\n",
       " 'o',\n",
       " 'c',\n",
       " 'c',\n",
       " '(',\n",
       " 'o',\n",
       " 'c',\n",
       " 'o',\n",
       " '6',\n",
       " 'o',\n",
       " '(',\n",
       " ')',\n",
       " '8',\n",
       " 'o',\n",
       " '9',\n",
       " 'c',\n",
       " '5',\n",
       " '1',\n",
       " 'n',\n",
       " 'c',\n",
       " 'c',\n",
       " ')',\n",
       " 'c',\n",
       " 'si',\n",
       " 'c',\n",
       " 'c',\n",
       " '-',\n",
       " 'c',\n",
       " 'c',\n",
       " 'o',\n",
       " '=',\n",
       " '[*]',\n",
       " '1',\n",
       " '-',\n",
       " '2',\n",
       " 'c',\n",
       " '[*]',\n",
       " '4',\n",
       " 'c',\n",
       " 'c',\n",
       " 'c',\n",
       " 'c',\n",
       " 'c',\n",
       " ')',\n",
       " 'c',\n",
       " '1',\n",
       " 'c',\n",
       " 'c',\n",
       " 'c',\n",
       " 'c',\n",
       " '(',\n",
       " 'c',\n",
       " '(',\n",
       " '(',\n",
       " 'o',\n",
       " 'c',\n",
       " 'c',\n",
       " '(',\n",
       " 'c',\n",
       " 'c',\n",
       " ')',\n",
       " '▁',\n",
       " '2',\n",
       " 'n',\n",
       " 'c',\n",
       " 'n',\n",
       " '(',\n",
       " '5',\n",
       " ')',\n",
       " 'c',\n",
       " 'c',\n",
       " '(',\n",
       " 'c',\n",
       " ')']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ground_truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForMaskedLM were not initialized from the model checkpoint at kuelumbus/polyBERT and are newly initialized: ['cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([153, 153,  62,  62, 111,  10,  35,  41, 195,  27,  42,  35, 152,  62,\n",
      "        229, 166,  21,  42, 200, 166, 166,  36,  27, 229,  35, 264, 166, 229,\n",
      "        166,  22,  35, 229, 229, 166,  27,  27, 229,  62,  62,  25,  27, 195,\n",
      "         18,  34,  80, 153,  64,  20,  80, 164, 195, 166,  16,  21, 166,  21,\n",
      "        153, 200, 229, 153, 161,  62, 172, 229,  21, 229,  42,  42, 229, 229,\n",
      "        166,  32, 153, 240,  35, 229, 229, 229, 209, 161, 166, 166,  21, 166,\n",
      "         10,  32, 111, 195,  34, 161,  34,  43, 111, 111,  62, 111, 111,  27,\n",
      "        229,  42, 195,  62, 229,   8,   7,  41,  10,  25, 229,  27, 151, 152,\n",
      "        200,   7,  10,  10,  36,  11, 229,  23,  34, 153,  34, 161, 153,  42,\n",
      "        161, 195,  27,  11, 153,  42,  42,  42, 166, 229, 195,  20,  25, 195,\n",
      "         80,   7, 111, 153, 229, 111,  42,  11,  42, 195, 229,  80, 153,  34,\n",
      "         48, 152, 166,  42,  27, 166, 161,  19,  27, 161, 195, 195,  62,  34,\n",
      "        161,  28, 111,  43, 161,  42,  48,  48,  22, 153,  42,  10,   5,  14,\n",
      "         14, 111,  14,  34, 111,  21,  62,  34, 166,  20,  25,  34, 151,   8,\n",
      "        153,  43,  42,  42, 166,  34, 229, 229,  44, 195,   5, 229, 229,  27,\n",
      "        151,  62, 111, 111, 153, 152, 229,  34, 152, 152,  26,  27,  27,  21,\n",
      "         42,  48, 166,  33,  21, 266, 166, 229,  42, 195,  27, 153, 195, 229,\n",
      "         35,  43, 153, 200, 166,  42, 153,  42,  42,  42, 153,  23, 113, 153,\n",
      "         42, 229,  42, 166,  22,  42,  35, 152,  11,  26,   7,  62,  62,  27,\n",
      "        111,  43,  36,  26,  24, 152, 202], device='cuda:0')\n",
      "pretrained F1 Score: 0.01098901098901099\n"
     ]
    }
   ],
   "source": [
    "# Load tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained('kuelumbus/polyBERT')\n",
    "model = AutoModelForMaskedLM.from_pretrained('kuelumbus/polyBERT').to(device)\n",
    "# tokenizer = DebertaV2Tokenizer(f\"spm_{size}.model\",f\"spm_{size}.vocab\")\n",
    "# model = DebertaV2ForMaskedLM.from_pretrained(f'model_{size}_final/').to(device)\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Mask 15% of tokens of each string in test data\n",
    "masked_psmiles, ground_truth = create_masked_test_set(tokenizer,psmiles_strings)\n",
    "\n",
    "# Tokenize the sentences\n",
    "inputs = tokenizer(masked_psmiles, return_tensors='pt', padding=True)\n",
    "inputs = inputs.to(device)\n",
    "\n",
    "# Run inference to get predictions for masked tokens\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "    predictions = outputs.logits\n",
    "\n",
    "# Get the predicted token IDs for the masked positions\n",
    "masked_indices = (inputs.input_ids == tokenizer.mask_token_id).nonzero(as_tuple=True)\n",
    "predicted_token_ids = predictions[masked_indices].argmax(dim=-1)\n",
    "print(predicted_token_ids)\n",
    "\n",
    "# Convert predicted token IDs back to words\n",
    "predicted_tokens = tokenizer.convert_ids_to_tokens(predicted_token_ids)\n",
    "\n",
    "# Convert true tokens to token IDs\n",
    "true_token_ids = tokenizer.convert_tokens_to_ids(ground_truth)\n",
    "\n",
    "# Compute F1 score (using token IDs for comparison)\n",
    "f1 = f1_score(true_token_ids, predicted_token_ids.cpu().numpy(), average='micro')\n",
    "\n",
    "print(f\"pretrained F1 Score: {f1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaseModelOutput(last_hidden_state=tensor([[[ 1.6763,  0.0287,  0.4598,  ..., -1.2926,  0.6870, -0.7143],\n",
       "         [ 1.6320,  0.0815,  0.3851,  ..., -1.3084,  0.6598, -0.6348],\n",
       "         [ 1.7983, -0.8584,  1.2366,  ..., -1.5453,  0.6853, -0.3230],\n",
       "         ...,\n",
       "         [ 0.2695,  0.6533,  0.5087,  ..., -0.1991, -0.1496,  0.5560],\n",
       "         [ 0.2695,  0.6533,  0.5087,  ..., -0.1991, -0.1496,  0.5560],\n",
       "         [ 0.2695,  0.6533,  0.5087,  ..., -0.1991, -0.1496,  0.5560]],\n",
       "\n",
       "        [[ 1.1026,  0.1546, -0.7523,  ..., -1.7872,  1.5069,  0.1879],\n",
       "         [ 1.1024,  0.1513, -0.7469,  ..., -1.8025,  1.5083,  0.1821],\n",
       "         [ 1.6554, -0.7777,  0.3114,  ..., -2.1281,  1.0336,  0.2962],\n",
       "         ...,\n",
       "         [ 0.2695,  0.6533,  0.5087,  ..., -0.1991, -0.1496,  0.5560],\n",
       "         [ 0.2695,  0.6533,  0.5087,  ..., -0.1991, -0.1496,  0.5560],\n",
       "         [ 0.2695,  0.6533,  0.5087,  ..., -0.1991, -0.1496,  0.5560]],\n",
       "\n",
       "        [[ 0.6098,  0.5521,  0.7161,  ..., -0.1110,  0.4773,  0.2431],\n",
       "         [ 0.6068,  0.5573,  0.7232,  ..., -0.1014,  0.4782,  0.2279],\n",
       "         [ 1.0595,  0.5479,  1.4700,  ..., -0.9933,  0.4818,  1.0918],\n",
       "         ...,\n",
       "         [ 0.2695,  0.6533,  0.5087,  ..., -0.1991, -0.1496,  0.5560],\n",
       "         [ 0.2695,  0.6533,  0.5087,  ..., -0.1991, -0.1496,  0.5560],\n",
       "         [ 0.2695,  0.6533,  0.5087,  ..., -0.1991, -0.1496,  0.5560]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 0.8711,  1.5313,  0.7226,  ..., -1.2256,  0.6931,  1.0683],\n",
       "         [ 0.8625,  1.5370,  0.7564,  ..., -1.2272,  0.6996,  1.0779],\n",
       "         [ 0.8369,  0.5811,  1.3325,  ..., -1.7533,  0.4667,  1.1085],\n",
       "         ...,\n",
       "         [ 0.2695,  0.6533,  0.5087,  ..., -0.1991, -0.1496,  0.5560],\n",
       "         [ 0.2695,  0.6533,  0.5087,  ..., -0.1991, -0.1496,  0.5560],\n",
       "         [ 0.2695,  0.6533,  0.5087,  ..., -0.1991, -0.1496,  0.5560]],\n",
       "\n",
       "        [[ 0.4394,  0.3522, -0.2668,  ..., -0.4546,  1.4107,  0.4586],\n",
       "         [ 0.4348,  0.3488, -0.2468,  ..., -0.4892,  1.4075,  0.4562],\n",
       "         [ 0.6644, -0.5998,  0.8802,  ..., -0.6007,  0.7914,  0.2906],\n",
       "         ...,\n",
       "         [ 0.2695,  0.6533,  0.5087,  ..., -0.1991, -0.1496,  0.5560],\n",
       "         [ 0.2695,  0.6533,  0.5087,  ..., -0.1991, -0.1496,  0.5560],\n",
       "         [ 0.2695,  0.6533,  0.5087,  ..., -0.1991, -0.1496,  0.5560]],\n",
       "\n",
       "        [[ 0.3012,  0.4365, -0.2935,  ..., -1.0723,  0.9067,  0.4785],\n",
       "         [ 0.2733,  0.4420, -0.2929,  ..., -1.0591,  0.9213,  0.4837],\n",
       "         [ 0.5669,  0.0338,  0.5420,  ..., -1.0351,  0.7065,  0.7240],\n",
       "         ...,\n",
       "         [ 0.2695,  0.6533,  0.5087,  ..., -0.1991, -0.1496,  0.5560],\n",
       "         [ 0.2695,  0.6533,  0.5087,  ..., -0.1991, -0.1496,  0.5560],\n",
       "         [ 0.2695,  0.6533,  0.5087,  ..., -0.1991, -0.1496,  0.5560]]],\n",
       "       device='cuda:0'), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[151,\n",
       " 151,\n",
       " 151,\n",
       " 7,\n",
       " 13,\n",
       " 151,\n",
       " 151,\n",
       " 6,\n",
       " 151,\n",
       " 151,\n",
       " 15,\n",
       " 14,\n",
       " 6,\n",
       " 151,\n",
       " 17,\n",
       " 7,\n",
       " 8,\n",
       " 35,\n",
       " 5,\n",
       " 33,\n",
       " 7,\n",
       " 8,\n",
       " 35,\n",
       " 7,\n",
       " 7,\n",
       " 6,\n",
       " 34,\n",
       " 8,\n",
       " 7,\n",
       " 151,\n",
       " 33,\n",
       " 33,\n",
       " 151,\n",
       " 264,\n",
       " 151,\n",
       " 161,\n",
       " 33,\n",
       " 15,\n",
       " 151,\n",
       " 15,\n",
       " 33,\n",
       " 13,\n",
       " 7,\n",
       " 33,\n",
       " 5,\n",
       " 151,\n",
       " 6,\n",
       " 151,\n",
       " 151,\n",
       " 7,\n",
       " 7,\n",
       " 15,\n",
       " 151,\n",
       " 151,\n",
       " 6,\n",
       " 21,\n",
       " 151,\n",
       " 6,\n",
       " 7,\n",
       " 6,\n",
       " 7,\n",
       " 151,\n",
       " 151,\n",
       " 151,\n",
       " 14,\n",
       " 151,\n",
       " 7,\n",
       " 151,\n",
       " 13,\n",
       " 151,\n",
       " 151,\n",
       " 14,\n",
       " 152,\n",
       " 16,\n",
       " 8,\n",
       " 151,\n",
       " 151,\n",
       " 48,\n",
       " 18,\n",
       " 151,\n",
       " 151,\n",
       " 151,\n",
       " 7,\n",
       " 151,\n",
       " 13,\n",
       " 12,\n",
       " 151,\n",
       " 6,\n",
       " 151,\n",
       " 6,\n",
       " 16,\n",
       " 15,\n",
       " 16,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 12,\n",
       " 151,\n",
       " 21,\n",
       " 33,\n",
       " 8,\n",
       " 33,\n",
       " 151,\n",
       " 15,\n",
       " 152,\n",
       " 7,\n",
       " 151,\n",
       " 14,\n",
       " 264,\n",
       " 151,\n",
       " 6,\n",
       " 35,\n",
       " 34,\n",
       " 8,\n",
       " 151,\n",
       " 151,\n",
       " 12,\n",
       " 151,\n",
       " 151,\n",
       " 6,\n",
       " 151,\n",
       " 151,\n",
       " 6,\n",
       " 26,\n",
       " 14,\n",
       " 35,\n",
       " 7,\n",
       " 13,\n",
       " 5,\n",
       " 7,\n",
       " 62,\n",
       " 6,\n",
       " 151,\n",
       " 62,\n",
       " 6,\n",
       " 6,\n",
       " 7,\n",
       " 21,\n",
       " 6,\n",
       " 16,\n",
       " 7,\n",
       " 34,\n",
       " 151,\n",
       " 33,\n",
       " 151,\n",
       " 151,\n",
       " 7,\n",
       " 151,\n",
       " 17,\n",
       " 7,\n",
       " 62,\n",
       " 62,\n",
       " 151,\n",
       " 264,\n",
       " 151,\n",
       " 151,\n",
       " 151,\n",
       " 151,\n",
       " 151,\n",
       " 151,\n",
       " 151,\n",
       " 151,\n",
       " 6,\n",
       " 16,\n",
       " 17,\n",
       " 7,\n",
       " 13,\n",
       " 151,\n",
       " 151,\n",
       " 33,\n",
       " 33,\n",
       " 6,\n",
       " 33,\n",
       " 151,\n",
       " 151,\n",
       " 12,\n",
       " 21,\n",
       " 12,\n",
       " 151,\n",
       " 6,\n",
       " 7,\n",
       " 151,\n",
       " 6,\n",
       " 151,\n",
       " 33,\n",
       " 33,\n",
       " 151,\n",
       " 151,\n",
       " 20,\n",
       " 151,\n",
       " 8,\n",
       " 33,\n",
       " 264,\n",
       " 6,\n",
       " 151,\n",
       " 43,\n",
       " 7,\n",
       " 151,\n",
       " 151,\n",
       " 6,\n",
       " 33,\n",
       " 34,\n",
       " 35,\n",
       " 6,\n",
       " 151,\n",
       " 6,\n",
       " 151,\n",
       " 7,\n",
       " 18,\n",
       " 151,\n",
       " 33,\n",
       " 15,\n",
       " 151,\n",
       " 7,\n",
       " 151,\n",
       " 151,\n",
       " 33,\n",
       " 151,\n",
       " 6,\n",
       " 151,\n",
       " 21,\n",
       " 7,\n",
       " 151,\n",
       " 21,\n",
       " 152,\n",
       " 35,\n",
       " 7,\n",
       " 13,\n",
       " 12,\n",
       " 151,\n",
       " 151,\n",
       " 13,\n",
       " 151,\n",
       " 13,\n",
       " 7,\n",
       " 33,\n",
       " 151,\n",
       " 7,\n",
       " 34,\n",
       " 13,\n",
       " 151,\n",
       " 151,\n",
       " 151,\n",
       " 151,\n",
       " 7,\n",
       " 264,\n",
       " 5,\n",
       " 33,\n",
       " 151,\n",
       " 151,\n",
       " 151,\n",
       " 7,\n",
       " 151,\n",
       " 8,\n",
       " 151,\n",
       " 34,\n",
       " 151,\n",
       " 151,\n",
       " 151,\n",
       " 151,\n",
       " 151,\n",
       " 33,\n",
       " 14,\n",
       " 7,\n",
       " 151,\n",
       " 151,\n",
       " 151,\n",
       " 151,\n",
       " 151,\n",
       " 6,\n",
       " 151,\n",
       " 151,\n",
       " 6,\n",
       " 34,\n",
       " 33,\n",
       " 151,\n",
       " 151,\n",
       " 151,\n",
       " 7,\n",
       " 33,\n",
       " 15,\n",
       " 35,\n",
       " 7]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true_token_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/hm62/hl4138/polybert-venv/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  1,  1,\n",
      "         1,  1,  1,  1,  1,  1,  1,  1,  2,  2,  2,  2,  2,  3,  3,  3,  3,  3,\n",
      "         3,  3,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  5,  5,\n",
      "         5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  6,  6,  6,  6,  6,  6,  6,  6,\n",
      "         6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  7,  7,  7,  7,  7,  7,\n",
      "         7,  7,  7,  7,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  9,  9,  9,\n",
      "         9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9, 10, 10, 10, 10,\n",
      "        10, 10, 10, 10, 10, 10, 10, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 12,\n",
      "        12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 13,\n",
      "        13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13,\n",
      "        14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 15, 15, 15, 15,\n",
      "        15, 15, 15, 15, 15, 16, 16, 16, 16, 16, 16, 17, 17, 17, 18, 18, 18, 18,\n",
      "        18, 18, 18, 18, 18, 18, 18, 18, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19,\n",
      "        19, 19, 19], device='cuda:0'), tensor([  1,   2,   9,  17,  20,  26,  34,  43,  65,  69,  72,  84,  88,  97,\n",
      "        100, 101,   9,  25,  39,  44,  48,  55,  67,  71,  77,  81,  23,  24,\n",
      "         39,  40,  45,   3,  15,  22,  26,  37,  47,  55,   1,   4,   5,  12,\n",
      "         16,  19,  24,  28,  37,  41,  45,  52,  93,  98,   1,   7,  10,  17,\n",
      "         31,  34,  39,  40,  53,  59,  65,  70,   1,  10,  13,  14,  21,  24,\n",
      "         28,  32,  36,  40,  43,  47,  50,  53,  65,  69,  76,  86,  92,  96,\n",
      "          4,   5,   8,   9,  14,  19,  25,  49,  62,  66,   4,  10,  15,  18,\n",
      "         24,  30,  39,  46,  79, 100, 101,  44,  49,  61,  70,  84,  87,  94,\n",
      "         98,  99, 100, 126, 131, 134, 138, 147, 148, 149,   4,  13,  45,  54,\n",
      "         55,  62,  65,  66,  78,  90, 104,   1,   8,  16,  17,  21,  32,  43,\n",
      "         53,  73,  78,   7,  25,  28,  33,  43,  50,  70,  73,  74,  75,  76,\n",
      "         80,  85,  90, 100, 106, 128, 139,   7,  16,  17,  18,  28,  29,  38,\n",
      "         43,  59,  63,  69,  78,  81,  85,  94,  99, 112, 127, 128,   4,   5,\n",
      "         10,  16,  21,  22,  47,  64,  70,  79,  83,  94,  98, 106,   1,   5,\n",
      "         11,  14,  20,  52,  56,  59,  67,  10,  14,  17,  28,  33,  43,  11,\n",
      "         14,  20,   5,   9,  20,  25,  36,  44,  49,  50,  51,  55,  60,  69,\n",
      "          1,   9,  10,  23,  32,  33,  43,  49,  55,  66,  67,  88,  93],\n",
      "       device='cuda:0'))\n",
      "pretrained1M F1 Score: 0.5232067510548524\n"
     ]
    }
   ],
   "source": [
    "# Load tokenizer and model\n",
    "# tokenizer = AutoTokenizer.from_pretrained('kuelumbus/polyBERT')\n",
    "# model = AutoModelForMaskedLM.from_pretrained('kuelumbus/polyBERT').to(device)\n",
    "tokenizer = DebertaV2Tokenizer(f\"spm_{size}.model\",f\"spm_{size}.vocab\")\n",
    "model = DebertaV2ForMaskedLM.from_pretrained(f'model_{size}_final/').to(device)\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Mask 15% of tokens of each string in test data\n",
    "masked_psmiles, ground_truth = create_masked_test_set(tokenizer,psmiles_strings)\n",
    "\n",
    "# Tokenize the sentences\n",
    "inputs = tokenizer(masked_psmiles, return_tensors='pt', padding=True)\n",
    "inputs = inputs.to(device)\n",
    "\n",
    "# Run inference to get predictions for masked tokens\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "    predictions = outputs.logits\n",
    "\n",
    "# Get the predicted token IDs for the masked positions\n",
    "masked_indices = (inputs.input_ids == tokenizer.mask_token_id).nonzero(as_tuple=True)\n",
    "predicted_token_ids = predictions[masked_indices].argmax(dim=-1)\n",
    "print(masked_indices)\n",
    "\n",
    "# Convert predicted token IDs back to words\n",
    "predicted_tokens = tokenizer.convert_ids_to_tokens(predicted_token_ids)\n",
    "\n",
    "# Convert true tokens to token IDs\n",
    "true_token_ids = tokenizer.convert_tokens_to_ids(ground_truth)\n",
    "\n",
    "# Compute F1 score (using token IDs for comparison)\n",
    "f1 = f1_score(true_token_ids, predicted_token_ids.cpu().numpy(), average='micro')\n",
    "\n",
    "print(f\"pretrained1M F1 Score: {f1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
